2023-01-14 10:49:06.693308 PST | [name-of-experiment_2023_01_14_10_48_58_0000--s-0] Epoch 0 finished
---------------------------------------  ---------------
epoch                                        0
replay_buffer/size                       11000
trainer/QF Loss                              0.159712
trainer/Policy Loss                          0.00169313
trainer/Raw Policy Loss                      0.00169313
trainer/Preactivation Policy Loss            0
trainer/Q Predictions Mean                  -0.00165358
trainer/Q Predictions Std                    0.00138997
trainer/Q Predictions Max                    0.00142113
trainer/Q Predictions Min                   -0.00584788
trainer/Q Targets Mean                      -0.172055
trainer/Q Targets Std                        0.361501
trainer/Q Targets Max                        0.858138
trainer/Q Targets Min                       -1.36622
trainer/Bellman Errors Mean                  0.159712
trainer/Bellman Errors Std                   0.266396
trainer/Bellman Errors Max                   1.85693
trainer/Bellman Errors Min                   2.92464e-05
trainer/Policy Action Mean                   2.86334e-05
trainer/Policy Action Std                    0.00192562
trainer/Policy Action Max                    0.00807438
trainer/Policy Action Min                   -0.00652937
expl/num steps total                     11000
expl/num paths total                        11
expl/path length Mean                     1000
expl/path length Std                         0
expl/path length Max                      1000
expl/path length Min                      1000
expl/Rewards Mean                           -0.146462
expl/Rewards Std                             0.335353
expl/Rewards Max                             0.990807
expl/Rewards Min                            -1.5097
expl/Returns Mean                         -146.462
expl/Returns Std                             0
expl/Returns Max                          -146.462
expl/Returns Min                          -146.462
expl/Actions Mean                            0.041965
expl/Actions Std                             0.52031
expl/Actions Max                             1
expl/Actions Min                            -1
expl/Num Paths                               1
expl/Average Returns                      -146.462
expl/env_infos/final/reward_run Mean        -0.341111
expl/env_infos/final/reward_run Std          0
expl/env_infos/final/reward_run Max         -0.341111
expl/env_infos/final/reward_run Min         -0.341111
expl/env_infos/initial/reward_run Mean       0.272814
expl/env_infos/initial/reward_run Std        0
expl/env_infos/initial/reward_run Max        0.272814
expl/env_infos/initial/reward_run Min        0.272814
expl/env_infos/reward_run Mean               0.017028
expl/env_infos/reward_run Std                0.328772
expl/env_infos/reward_run Max                1.17145
expl/env_infos/reward_run Min               -1.39483
expl/env_infos/final/reward_ctrl Mean       -0.169618
expl/env_infos/final/reward_ctrl Std         0
expl/env_infos/final/reward_ctrl Max        -0.169618
expl/env_infos/final/reward_ctrl Min        -0.169618
expl/env_infos/initial/reward_ctrl Mean     -0.00671638
expl/env_infos/initial/reward_ctrl Std       0
expl/env_infos/initial/reward_ctrl Max      -0.00671638
expl/env_infos/initial/reward_ctrl Min      -0.00671638
expl/env_infos/reward_ctrl Mean             -0.16349
expl/env_infos/reward_ctrl Std               0.075948
expl/env_infos/reward_ctrl Max              -0.00671638
expl/env_infos/reward_ctrl Min              -0.414137
eval/num steps total                      1000
eval/num paths total                         1
eval/path length Mean                     1000
eval/path length Std                         0
eval/path length Max                      1000
eval/path length Min                      1000
eval/Rewards Mean                            1.61481e-05
eval/Rewards Std                             0.0158173
eval/Rewards Max                             0.193381
eval/Rewards Min                            -0.230389
eval/Returns Mean                            0.0161481
eval/Returns Std                             0
eval/Returns Max                             0.0161481
eval/Returns Min                             0.0161481
eval/Actions Mean                           -2.82408e-05
eval/Actions Std                             9.91192e-05
eval/Actions Max                             0.0011844
eval/Actions Min                            -0.00155529
eval/Num Paths                               1
eval/Average Returns                         0.0161481
eval/env_infos/final/reward_run Mean        -1.38778e-16
eval/env_infos/final/reward_run Std          0
eval/env_infos/final/reward_run Max         -1.38778e-16
eval/env_infos/final/reward_run Min         -1.38778e-16
eval/env_infos/initial/reward_run Mean       0.158567
eval/env_infos/initial/reward_run Std        0
eval/env_infos/initial/reward_run Max        0.158567
eval/env_infos/initial/reward_run Min        0.158567
eval/env_infos/reward_run Mean               1.61544e-05
eval/env_infos/reward_run Std                0.0158173
eval/env_infos/reward_run Max                0.193381
eval/env_infos/reward_run Min               -0.230389
eval/env_infos/final/reward_ctrl Mean       -5.20194e-09
eval/env_infos/final/reward_ctrl Std         0
eval/env_infos/final/reward_ctrl Max        -5.20194e-09
eval/env_infos/final/reward_ctrl Min        -5.20194e-09
eval/env_infos/initial/reward_ctrl Mean     -4.69679e-09
eval/env_infos/initial/reward_ctrl Std       0
eval/env_infos/initial/reward_ctrl Max      -4.69679e-09
eval/env_infos/initial/reward_ctrl Min      -4.69679e-09
eval/env_infos/reward_ctrl Mean             -6.37335e-09
eval/env_infos/reward_ctrl Std               1.62003e-08
eval/env_infos/reward_ctrl Max              -3.26992e-09
eval/env_infos/reward_ctrl Min              -3.93471e-07
time/data storing (s)                        0.00349579
time/evaluation sampling (s)                 2.31486
time/exploration sampling (s)                0.228265
time/logging (s)                             0.00462845
time/saving (s)                              0.00542081
time/training (s)                            5.69535
time/epoch (s)                               8.25201
time/total (s)                               8.28515
Epoch                                        0
---------------------------------------  ---------------
2023-01-14 10:49:12.589588 PST | [name-of-experiment_2023_01_14_10_48_58_0000--s-0] Epoch 1 finished
---------------------------------------  ---------------
epoch                                        1
replay_buffer/size                       12000
trainer/QF Loss                              0.140762
trainer/Policy Loss                         -0.718244
trainer/Raw Policy Loss                     -0.718244
trainer/Preactivation Policy Loss            0
trainer/Q Predictions Mean                   0.452351
trainer/Q Predictions Std                    2.3769
trainer/Q Predictions Max                    7.56832
trainer/Q Predictions Min                   -5.71228
trainer/Q Targets Mean                       0.443483
trainer/Q Targets Std                        2.34538
trainer/Q Targets Max                        7.57655
trainer/Q Targets Min                       -4.36158
trainer/Bellman Errors Mean                  0.140762
trainer/Bellman Errors Std                   0.317959
trainer/Bellman Errors Max                   1.92949
trainer/Bellman Errors Min                   2.58377e-07
trainer/Policy Action Mean                  -0.0291596
trainer/Policy Action Std                    0.480974
trainer/Policy Action Max                    0.999911
trainer/Policy Action Min                   -1
expl/num steps total                     12000
expl/num paths total                        12
expl/path length Mean                     1000
expl/path length Std                         0
expl/path length Max                      1000
expl/path length Min                      1000
expl/Rewards Mean                           -0.167812
expl/Rewards Std                             0.377525
expl/Rewards Max                             1.27881
expl/Rewards Min                            -1.53228
expl/Returns Mean                         -167.812
expl/Returns Std                             0
expl/Returns Max                          -167.812
expl/Returns Min                          -167.812
expl/Actions Mean                           -0.0944044
expl/Actions Std                             0.637949
expl/Actions Max                             1
expl/Actions Min                            -1
expl/Num Paths                               1
expl/Average Returns                      -167.812
expl/env_infos/final/reward_run Mean        -0.250842
expl/env_infos/final/reward_run Std          0
expl/env_infos/final/reward_run Max         -0.250842
expl/env_infos/final/reward_run Min         -0.250842
expl/env_infos/initial/reward_run Mean       0.945756
expl/env_infos/initial/reward_run Std        0
expl/env_infos/initial/reward_run Max        0.945756
expl/env_infos/initial/reward_run Min        0.945756
expl/env_infos/reward_run Mean               0.0817229
expl/env_infos/reward_run Std                0.374027
expl/env_infos/reward_run Max                1.63062
expl/env_infos/reward_run Min               -1.28992
expl/env_infos/final/reward_ctrl Mean       -0.305381
expl/env_infos/final/reward_ctrl Std         0
expl/env_infos/final/reward_ctrl Max        -0.305381
expl/env_infos/final/reward_ctrl Min        -0.305381
expl/env_infos/initial/reward_ctrl Mean     -0.163579
expl/env_infos/initial/reward_ctrl Std       0
expl/env_infos/initial/reward_ctrl Max      -0.163579
expl/env_infos/initial/reward_ctrl Min      -0.163579
expl/env_infos/reward_ctrl Mean             -0.249535
expl/env_infos/reward_ctrl Std               0.0904342
expl/env_infos/reward_ctrl Max              -0.01717
expl/env_infos/reward_ctrl Min              -0.511586
eval/num steps total                      2000
eval/num paths total                         2
eval/path length Mean                     1000
eval/path length Std                         0
eval/path length Max                      1000
eval/path length Min                      1000
eval/Rewards Mean                           -0.115936
eval/Rewards Std                             0.142661
eval/Rewards Max                             0.713656
eval/Rewards Min                            -1.44102
eval/Returns Mean                         -115.936
eval/Returns Std                             0
eval/Returns Max                          -115.936
eval/Returns Min                          -115.936
eval/Actions Mean                           -0.116257
eval/Actions Std                             0.395088
eval/Actions Max                             0.999674
eval/Actions Min                            -0.99924
eval/Num Paths                               1
eval/Average Returns                      -115.936
eval/env_infos/final/reward_run Mean        -8.96314e-09
eval/env_infos/final/reward_run Std          0
eval/env_infos/final/reward_run Max         -8.96314e-09
eval/env_infos/final/reward_run Min         -8.96314e-09
eval/env_infos/initial/reward_run Mean       0.362762
eval/env_infos/initial/reward_run Std        0
eval/env_infos/initial/reward_run Max        0.362762
eval/env_infos/initial/reward_run Min        0.362762
eval/env_infos/reward_run Mean              -0.01417
eval/env_infos/reward_run Std                0.148651
eval/env_infos/reward_run Max                0.879561
eval/env_infos/reward_run Min               -1.4016
eval/env_infos/final/reward_ctrl Mean       -0.102512
eval/env_infos/final/reward_ctrl Std         0
eval/env_infos/final/reward_ctrl Max        -0.102512
eval/env_infos/final/reward_ctrl Min        -0.102512
eval/env_infos/initial/reward_ctrl Mean     -0.0272884
eval/env_infos/initial/reward_ctrl Std       0
eval/env_infos/initial/reward_ctrl Max      -0.0272884
eval/env_infos/initial/reward_ctrl Min      -0.0272884
eval/env_infos/reward_ctrl Mean             -0.101766
eval/env_infos/reward_ctrl Std               0.0210445
eval/env_infos/reward_ctrl Max              -0.00202895
eval/env_infos/reward_ctrl Min              -0.301686
time/data storing (s)                        0.0032834
time/evaluation sampling (s)                 0.184601
time/exploration sampling (s)                0.198927
time/logging (s)                             0.00496095
time/saving (s)                              0.00134337
time/training (s)                            5.49923
time/epoch (s)                               5.89234
time/total (s)                              14.1814
Epoch                                        1
---------------------------------------  ---------------
2023-01-14 10:49:19.226998 PST | [name-of-experiment_2023_01_14_10_48_58_0000--s-0] Epoch 2 finished
---------------------------------------  ---------------
epoch                                        2
replay_buffer/size                       13000
trainer/QF Loss                              0.911783
trainer/Policy Loss                         -9.53321
trainer/Raw Policy Loss                     -9.53321
trainer/Preactivation Policy Loss            0
trainer/Q Predictions Mean                   7.3103
trainer/Q Predictions Std                    3.16716
trainer/Q Predictions Max                   17.4145
trainer/Q Predictions Min                    1.02414
trainer/Q Targets Mean                       7.13073
trainer/Q Targets Std                        3.24048
trainer/Q Targets Max                       17.7003
trainer/Q Targets Min                        0.175062
trainer/Bellman Errors Mean                  0.911783
trainer/Bellman Errors Std                   2.83802
trainer/Bellman Errors Max                  27.274
trainer/Bellman Errors Min                   2.75122e-09
trainer/Policy Action Mean                   0.338858
trainer/Policy Action Std                    0.873167
trainer/Policy Action Max                    1
trainer/Policy Action Min                   -1
expl/num steps total                     13000
expl/num paths total                        13
expl/path length Mean                     1000
expl/path length Std                         0
expl/path length Max                      1000
expl/path length Min                      1000
expl/Rewards Mean                           -0.459748
expl/Rewards Std                             0.322099
expl/Rewards Max                             0.451847
expl/Rewards Min                            -1.74471
expl/Returns Mean                         -459.748
expl/Returns Std                             0
expl/Returns Max                          -459.748
expl/Returns Min                          -459.748
expl/Actions Mean                            0.255088
expl/Actions Std                             0.776006
expl/Actions Max                             1
expl/Actions Min                            -1
expl/Num Paths                               1
expl/Average Returns                      -459.748
expl/env_infos/final/reward_run Mean         0.099798
expl/env_infos/final/reward_run Std          0
expl/env_infos/final/reward_run Max          0.099798
expl/env_infos/final/reward_run Min          0.099798
expl/env_infos/initial/reward_run Mean       0.280381
expl/env_infos/initial/reward_run Std        0
expl/env_infos/initial/reward_run Max        0.280381
expl/env_infos/initial/reward_run Min        0.280381
expl/env_infos/reward_run Mean              -0.0593954
expl/env_infos/reward_run Std                0.301986
expl/env_infos/reward_run Max                0.974307
expl/env_infos/reward_run Min               -1.25325
expl/env_infos/final/reward_ctrl Mean       -0.380843
expl/env_infos/final/reward_ctrl Std         0
expl/env_infos/final/reward_ctrl Max        -0.380843
expl/env_infos/final/reward_ctrl Min        -0.380843
expl/env_infos/initial/reward_ctrl Mean     -0.316192
expl/env_infos/initial/reward_ctrl Std       0
expl/env_infos/initial/reward_ctrl Max      -0.316192
expl/env_infos/initial/reward_ctrl Min      -0.316192
expl/env_infos/reward_ctrl Mean             -0.400353
expl/env_infos/reward_ctrl Std               0.0961055
expl/env_infos/reward_ctrl Max              -0.0765701
expl/env_infos/reward_ctrl Min              -0.6
eval/num steps total                      3000
eval/num paths total                         3
eval/path length Mean                     1000
eval/path length Std                         0
eval/path length Max                      1000
eval/path length Min                      1000
eval/Rewards Mean                           -0.619271
eval/Rewards Std                             0.366531
eval/Rewards Max                             0.206245
eval/Rewards Min                            -1.86201
eval/Returns Mean                         -619.271
eval/Returns Std                             0
eval/Returns Max                          -619.271
eval/Returns Min                          -619.271
eval/Actions Mean                            0.298164
eval/Actions Std                             0.887369
eval/Actions Max                             1
eval/Actions Min                            -1
eval/Num Paths                               1
eval/Average Returns                      -619.271
eval/env_infos/final/reward_run Mean        -0.469288
eval/env_infos/final/reward_run Std          0
eval/env_infos/final/reward_run Max         -0.469288
eval/env_infos/final/reward_run Min         -0.469288
eval/env_infos/initial/reward_run Mean       0.0613719
eval/env_infos/initial/reward_run Std        0
eval/env_infos/initial/reward_run Max        0.0613719
eval/env_infos/initial/reward_run Min        0.0613719
eval/env_infos/reward_run Mean              -0.0934754
eval/env_infos/reward_run Std                0.332093
eval/env_infos/reward_run Max                0.68141
eval/env_infos/reward_run Min               -1.26336
eval/env_infos/final/reward_ctrl Mean       -0.497653
eval/env_infos/final/reward_ctrl Std         0
eval/env_infos/final/reward_ctrl Max        -0.497653
eval/env_infos/final/reward_ctrl Min        -0.497653
eval/env_infos/initial/reward_ctrl Mean     -0.268387
eval/env_infos/initial/reward_ctrl Std       0
eval/env_infos/initial/reward_ctrl Max      -0.268387
eval/env_infos/initial/reward_ctrl Min      -0.268387
eval/env_infos/reward_ctrl Mean             -0.525795
eval/env_infos/reward_ctrl Std               0.0680039
eval/env_infos/reward_ctrl Max              -0.268387
eval/env_infos/reward_ctrl Min              -0.599901
time/data storing (s)                        0.00332467
time/evaluation sampling (s)                 0.186333
time/exploration sampling (s)                0.207686
time/logging (s)                             0.00506547
time/saving (s)                              0.0015819
time/training (s)                            6.22931
time/epoch (s)                               6.6333
time/total (s)                              20.8185
Epoch                                        2
---------------------------------------  ---------------
2023-01-14 10:49:25.405233 PST | [name-of-experiment_2023_01_14_10_48_58_0000--s-0] Epoch 3 finished
---------------------------------------  ---------------
epoch                                        3
replay_buffer/size                       14000
trainer/QF Loss                              0.998972
trainer/Policy Loss                        -13.7618
trainer/Raw Policy Loss                    -13.7618
trainer/Preactivation Policy Loss            0
trainer/Q Predictions Mean                  12.1721
trainer/Q Predictions Std                    6.57159
trainer/Q Predictions Max                   33.7009
trainer/Q Predictions Min                    5.71335
trainer/Q Targets Mean                      11.9542
trainer/Q Targets Std                        6.5231
trainer/Q Targets Max                       31.0471
trainer/Q Targets Min                        6.1727
trainer/Bellman Errors Mean                  0.998972
trainer/Bellman Errors Std                   2.23931
trainer/Bellman Errors Max                  18.9438
trainer/Bellman Errors Min                   0.000568798
trainer/Policy Action Mean                  -0.428506
trainer/Policy Action Std                    0.854947
trainer/Policy Action Max                    1
trainer/Policy Action Min                   -1
expl/num steps total                     14000
expl/num paths total                        14
expl/path length Mean                     1000
expl/path length Std                         0
expl/path length Max                      1000
expl/path length Min                      1000
expl/Rewards Mean                           -0.420401
expl/Rewards Std                             0.380747
expl/Rewards Max                             0.922964
expl/Rewards Min                            -2.07245
expl/Returns Mean                         -420.401
expl/Returns Std                             0
expl/Returns Max                          -420.401
expl/Returns Min                          -420.401
expl/Actions Mean                            0.151505
expl/Actions Std                             0.774113
expl/Actions Max                             1
expl/Actions Min                            -1
expl/Num Paths                               1
expl/Average Returns                      -420.401
expl/env_infos/final/reward_run Mean        -0.950164
expl/env_infos/final/reward_run Std          0
expl/env_infos/final/reward_run Max         -0.950164
expl/env_infos/final/reward_run Min         -0.950164
expl/env_infos/initial/reward_run Mean       0.262062
expl/env_infos/initial/reward_run Std        0
expl/env_infos/initial/reward_run Max        0.262062
expl/env_infos/initial/reward_run Min        0.262062
expl/env_infos/reward_run Mean              -0.047078
expl/env_infos/reward_run Std                0.361643
expl/env_infos/reward_run Max                1.12557
expl/env_infos/reward_run Min               -1.62973
expl/env_infos/final/reward_ctrl Mean       -0.458319
expl/env_infos/final/reward_ctrl Std         0
expl/env_infos/final/reward_ctrl Max        -0.458319
expl/env_infos/final/reward_ctrl Min        -0.458319
expl/env_infos/initial/reward_ctrl Mean     -0.453143
expl/env_infos/initial/reward_ctrl Std       0
expl/env_infos/initial/reward_ctrl Max      -0.453143
expl/env_infos/initial/reward_ctrl Min      -0.453143
expl/env_infos/reward_ctrl Mean             -0.373323
expl/env_infos/reward_ctrl Std               0.0953908
expl/env_infos/reward_ctrl Max              -0.101581
expl/env_infos/reward_ctrl Min              -0.6
eval/num steps total                      4000
eval/num paths total                         4
eval/path length Mean                     1000
eval/path length Std                         0
eval/path length Max                      1000
eval/path length Min                      1000
eval/Rewards Mean                           -0.451386
eval/Rewards Std                             0.0345447
eval/Rewards Max                             0.29379
eval/Rewards Min                            -0.811297
eval/Returns Mean                         -451.386
eval/Returns Std                             0
eval/Returns Max                          -451.386
eval/Returns Min                          -451.386
eval/Actions Mean                            0.465489
eval/Actions Std                             0.732368
eval/Actions Max                             1
eval/Actions Min                            -0.999954
eval/Num Paths                               1
eval/Average Returns                      -451.386
eval/env_infos/final/reward_run Mean        -9.17743e-09
eval/env_infos/final/reward_run Std          0
eval/env_infos/final/reward_run Max         -9.17743e-09
eval/env_infos/final/reward_run Min         -9.17743e-09
eval/env_infos/initial/reward_run Mean       0.0978515
eval/env_infos/initial/reward_run Std        0
eval/env_infos/initial/reward_run Max        0.0978515
eval/env_infos/initial/reward_run Min        0.0978515
eval/env_infos/reward_run Mean               0.000440357
eval/env_infos/reward_run Std                0.032226
eval/env_infos/reward_run Max                0.734342
eval/env_infos/reward_run Min               -0.351973
eval/env_infos/final/reward_ctrl Mean       -0.451702
eval/env_infos/final/reward_ctrl Std         0
eval/env_infos/final/reward_ctrl Max        -0.451702
eval/env_infos/final/reward_ctrl Min        -0.451702
eval/env_infos/initial/reward_ctrl Mean     -0.338445
eval/env_infos/initial/reward_ctrl Std       0
eval/env_infos/initial/reward_ctrl Max      -0.338445
eval/env_infos/initial/reward_ctrl Min      -0.338445
eval/env_infos/reward_ctrl Mean             -0.451826
eval/env_infos/reward_ctrl Std               0.00612121
eval/env_infos/reward_ctrl Max              -0.338445
eval/env_infos/reward_ctrl Min              -0.551552
time/data storing (s)                        0.00324879
time/evaluation sampling (s)                 0.259582
time/exploration sampling (s)                0.232242
time/logging (s)                             0.00488544
time/saving (s)                              0.00137497
time/training (s)                            5.67332
time/epoch (s)                               6.17465
time/total (s)                              26.9962
Epoch                                        3
---------------------------------------  ---------------
